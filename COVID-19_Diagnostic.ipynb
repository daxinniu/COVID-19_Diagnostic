{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"COVID-19_Diagnostic.ipynb","provenance":[{"file_id":"13K2JGzhObXHh0CnfBoGvzyCaG4ASlgD8","timestamp":1615396587824}],"collapsed_sections":[],"machine_shape":"hm"},"file_extension":".py","kernelspec":{"display_name":"Python 3","name":"python3"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"cells":[{"cell_type":"markdown","metadata":{"id":"WSMO2e0Ca3tU"},"source":["## **COVID-19 Diagnosis Using Chest X-Ray Data**\n","This project uses Chest X-Ray Data to train a deep neural network to help diagnose COVID-19."]},{"cell_type":"markdown","metadata":{"id":"n_Kecj9IbFCT"},"source":["### Introduction\n","COVID-19 is severely impacting the health of countless people worldwide. A crucial step in controlling the disease has been early detection of infected patients, which can be achieved through radiography, according to prior literature that shows COVID-19 causes chest abnormalities noticeable in chest X-rays."]},{"cell_type":"markdown","metadata":{"id":"u3tF64VybOiZ"},"source":["We begin by importing necessary packages for our model.\n"]},{"cell_type":"code","metadata":{"id":"ZKzt0tiAbYLY","executionInfo":{"status":"ok","timestamp":1638303302959,"user_tz":300,"elapsed":144,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["import cv2\n","import os\n","import numpy as np\n","import pandas as pd"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zd6HN-95LqYr"},"source":["###Data Collection\n","There is no substantially-sized, clinically verified, and publicly available COVID-19 dataset. However, a small composite dataset with X-Rays of COVID-19 positive patients recently became publicly available with [DeepCovid](https://github.com/shervinmin/DeepCovid), which compiled their data from:\n","\n","[Covid-Chestxray-Dataset](https://github.com/ieee8023/covid-chestxray-dataset) for COVID-19 X-ray samples\n","\n","[ChexPert Dataset](https://stanfordmlgroup.github.io/competitions/chexpert/) for Non-COVID samples\n"]},{"cell_type":"markdown","metadata":{"id":"zUMj440wKo7L"},"source":["Our data was stored on a google drive so we will mount the drive to get the data"]},{"cell_type":"code","metadata":{"id":"4VyDyG1pJHZ9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638303303105,"user_tz":300,"elapsed":5,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"8245168e-f7c5-4549-bdfc-a68a374b5e12"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"GaQiCm9YJbw0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638303308100,"user_tz":300,"elapsed":4998,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"1488e534-3f8d-4e9a-d969-71a6b92f3833"},"source":["from zipfile import ZipFile\n","\n","file_name = 'drive/My Drive/dataset.zip'\n","\n","with ZipFile(file_name, 'r') as zip:\n","  zip.extractall()\n","  print(\"Data extracted!\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Data extracted!\n"]}]},{"cell_type":"markdown","metadata":{"id":"4VmIvYDDRJR6"},"source":["Next, let us begin feature and label building. We can utilize Paths to \n","list the directory of every X-Ray from the dataset."]},{"cell_type":"code","metadata":{"id":"rO_jTgD5H1Zo","executionInfo":{"status":"ok","timestamp":1638303316623,"user_tz":300,"elapsed":8525,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["from imutils import paths\n","\n","data = []\n","labels = []\n","\n","# Grab list of image paths using paths.list_images\n","imagePaths = list(paths.list_images('data_upload_v3/train/covid')) + list(paths.list_images('data_upload_v3/train/non'))\n","\n","# Label and resize the images \n","\n","for imagePath in imagePaths:\n","\t# extract the class label from the filename\n","    if imagePath[21] == 'c':\n","\t    label = 'covid'\n","    else:\n","        label = 'normal'\n","\n","    image = cv2.imread(imagePath)\n","    image = cv2.resize(image, (224, 224))\n","\n","    data.append(image)\n","    labels.append(label)\n","\n","\n","# Convert data and labels to a Numpy Array and normalize the pixel values\n","data = np.array(data) / 255.0\n","labels = np.array(labels)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4_xQFmdt8Vz","executionInfo":{"status":"ok","timestamp":1638303316623,"user_tz":300,"elapsed":9,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"8b17276e-68de-4627-9f3b-605a1ab015fe"},"source":["(unique, counts) = np.unique(labels, return_counts=True)\n","frequencies = np.asarray((unique, counts)).T\n","print(frequencies)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[['covid' '84']\n"," ['normal' '2000']]\n"]}]},{"cell_type":"code","metadata":{"id":"tobHh4Pisnh5","executionInfo":{"status":"ok","timestamp":1638303326055,"user_tz":300,"elapsed":9439,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["# Extract test data\n","\n","test_data = []\n","test_labels = []\n","\n","# Grab list of image paths using paths.list_images\n","test_imagePaths = list(paths.list_images('data_upload_v3/train/covid')) + list(paths.list_images('data_upload_v3/train/non'))\n","\n","# Label and resize the images \n","\n","for imagePath in test_imagePaths:\n","\t# extract the class label from the filename\n","    if imagePath[21] == 'c':\n","\t    label = 'covid'\n","    else:\n","        label = 'normal'\n","\n","    image = cv2.imread(imagePath)\n","    image = cv2.resize(image, (224, 224))\n","\n","    test_data.append(image)\n","    test_labels.append(label)\n","\n","\n","# Convert data and labels to a Numpy Array and normalize the pixel values\n","test_data = np.array(test_data) / 255.0\n","test_labels = np.array(test_labels)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3zmzxdQd9ON"},"source":["Use SKLearn to one-hot encode our labels."]},{"cell_type":"code","metadata":{"id":"0J2yOK7md_y6","executionInfo":{"status":"ok","timestamp":1638303328894,"user_tz":300,"elapsed":2848,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.utils import to_categorical\n","\n","lb = LabelBinarizer()\n","labels = lb.fit_transform(labels)\n","labels = to_categorical(labels)\n","test_labels = lb.fit_transform(test_labels)\n","test_labels = to_categorical(test_labels)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHZEuVVDeBv9"},"source":["### Data Splitting\n","Now that we're done grabbing our data, we can begin to look at splitting the data into our training and validation sets."]},{"cell_type":"code","metadata":{"id":"FK0h4tnQeH56","executionInfo":{"status":"ok","timestamp":1638303329839,"user_tz":300,"elapsed":954,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["from sklearn.model_selection import train_test_split\n","# Partition data into 80% training and 20% validation\n","\n","trainX, testX, trainY, testY = train_test_split(data, labels, stratify=labels,test_size = 0.20, random_state=123)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_echi_0APEis"},"source":["### Set hyperparameters"]},{"cell_type":"code","metadata":{"id":"FweemVCFPH9c","executionInfo":{"status":"ok","timestamp":1638303329839,"user_tz":300,"elapsed":3,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["EPOCHS = 20\n","BATCH_SIZE = 16\n","LR = 1e-4"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YByvq-9oRwn_"},"source":["### Base model\n","We are gonna use ResNet as the base model. The pretrained weights come from imagenet, and our input is (224,224,3)."]},{"cell_type":"code","metadata":{"id":"InLljPNaeYI2","executionInfo":{"status":"ok","timestamp":1638303335321,"user_tz":300,"elapsed":5484,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["from tensorflow.keras.applications import ResNet152V2\n","from tensorflow.keras.layers import Input\n","\n","base = ResNet152V2(include_top=False, weights = 'imagenet', input_shape = (224,224,3))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B571TxNMebs_"},"source":["###Build Model###\n","We will construct our model here."]},{"cell_type":"code","metadata":{"id":"9B6da8LIehJ-","executionInfo":{"status":"ok","timestamp":1638303335479,"user_tz":300,"elapsed":165,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import AUC\n","from keras.losses import categorical_crossentropy\n","\n","#Construct structure after ResNet\n","head = base.output\n","head = AveragePooling2D(pool_size=(7,7))(head)\n","head = BatchNormalization()(head)\n","head = Flatten()(head)\n","head = Dense(128, activation='relu')(head)\n","head = Dense(2, activation='sigmoid')(head)\n","\n","# Combine the model\n","model = Model(inputs=base.input, outputs=head)\n","\n","# Freeze base layers\n","base.trainable = False\n","\n","# Compile the model\n","model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate = LR), metrics=[AUC(curve=\"PR\")])"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SoyFWCZeLe0"},"source":["### Data augmentation\n","Use data augmentation object for our training set to make the model more robust."]},{"cell_type":"code","metadata":{"id":"kzi3yvZYeSOy","executionInfo":{"status":"ok","timestamp":1638303335479,"user_tz":300,"elapsed":2,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","trainAug = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True\n",")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ataiZOiKe5du"},"source":["###Training\n"]},{"cell_type":"code","metadata":{"id":"cn-dHilyW5X9","executionInfo":{"status":"ok","timestamp":1638303335480,"user_tz":300,"elapsed":2,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}}},"source":["# Add Earlystopping to monitor validation loss\n","from tensorflow.keras.callbacks import EarlyStopping\n","callback = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 5)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpiC-2O3e780","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638303723251,"user_tz":300,"elapsed":387773,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"54c127f5-c8fb-4a01-a383-ddc740517b9d"},"source":["# Train model\n","H = model.fit(\n","    trainAug.flow(trainX, trainY, batch_size=BATCH_SIZE), \n","    validation_data = (testX, testY), \n","    steps_per_epoch=len(trainX) // BATCH_SIZE, \n","    epochs = EPOCHS,\n","    callbacks = [callback]\n",")"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 36s 235ms/step - loss: 0.3350 - auc: 0.8651 - val_loss: 0.1597 - val_auc: 0.9784\n","Epoch 2/20\n","104/104 [==============================] - 21s 199ms/step - loss: 0.1427 - auc: 0.9652 - val_loss: 0.1239 - val_auc: 0.9847\n","Epoch 3/20\n","104/104 [==============================] - 21s 199ms/step - loss: 0.1002 - auc: 0.9805 - val_loss: 0.1040 - val_auc: 0.9874\n","Epoch 4/20\n","104/104 [==============================] - 21s 200ms/step - loss: 0.0777 - auc: 0.9876 - val_loss: 0.0940 - val_auc: 0.9897\n","Epoch 5/20\n","104/104 [==============================] - 21s 198ms/step - loss: 0.0670 - auc: 0.9899 - val_loss: 0.0820 - val_auc: 0.9915\n","Epoch 6/20\n","104/104 [==============================] - 21s 198ms/step - loss: 0.0629 - auc: 0.9920 - val_loss: 0.0755 - val_auc: 0.9923\n","Epoch 7/20\n","104/104 [==============================] - 20s 196ms/step - loss: 0.0505 - auc: 0.9928 - val_loss: 0.0763 - val_auc: 0.9922\n","Epoch 8/20\n","104/104 [==============================] - 21s 198ms/step - loss: 0.0541 - auc: 0.9926 - val_loss: 0.0696 - val_auc: 0.9935\n","Epoch 9/20\n","104/104 [==============================] - 21s 199ms/step - loss: 0.0437 - auc: 0.9940 - val_loss: 0.0731 - val_auc: 0.9933\n","Epoch 10/20\n","104/104 [==============================] - 21s 198ms/step - loss: 0.0446 - auc: 0.9933 - val_loss: 0.0720 - val_auc: 0.9929\n","Epoch 11/20\n","104/104 [==============================] - 21s 197ms/step - loss: 0.0374 - auc: 0.9951 - val_loss: 0.0728 - val_auc: 0.9931\n","Epoch 12/20\n","104/104 [==============================] - 21s 197ms/step - loss: 0.0305 - auc: 0.9957 - val_loss: 0.0652 - val_auc: 0.9934\n","Epoch 13/20\n","104/104 [==============================] - 20s 196ms/step - loss: 0.0360 - auc: 0.9944 - val_loss: 0.0606 - val_auc: 0.9939\n","Epoch 14/20\n","104/104 [==============================] - 20s 196ms/step - loss: 0.0312 - auc: 0.9962 - val_loss: 0.0661 - val_auc: 0.9939\n","Epoch 15/20\n","104/104 [==============================] - 21s 197ms/step - loss: 0.0269 - auc: 0.9968 - val_loss: 0.0618 - val_auc: 0.9940\n","Epoch 16/20\n","104/104 [==============================] - 20s 196ms/step - loss: 0.0338 - auc: 0.9958 - val_loss: 0.0616 - val_auc: 0.9941\n","Epoch 17/20\n","104/104 [==============================] - 21s 198ms/step - loss: 0.0269 - auc: 0.9966 - val_loss: 0.0641 - val_auc: 0.9939\n","Epoch 18/20\n","104/104 [==============================] - 20s 197ms/step - loss: 0.0377 - auc: 0.9962 - val_loss: 0.0608 - val_auc: 0.9947\n"]}]},{"cell_type":"markdown","metadata":{"id":"-3I-K36we9tb"},"source":["### Predictions\n","Here are some important metrics to see how well our model performs on the test set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BeAFOFuX6ixE","executionInfo":{"status":"ok","timestamp":1638303732858,"user_tz":300,"elapsed":9627,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"d6381a2b-954a-4d48-ded2-9636903d18ad"},"source":["model.evaluate(test_data, test_labels)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["66/66 [==============================] - 9s 133ms/step - loss: 0.0267 - auc: 0.9979\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.026741014793515205, 0.9979087710380554]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"6mCKD5H_fEqi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638303738102,"user_tz":300,"elapsed":5265,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"f85a821d-ff08-4a15-b8c7-b746a7d209a1"},"source":["from sklearn.metrics import classification_report\n","\n","predIdxs = np.argmax(model.predict(testX, batch_size=BATCH_SIZE), axis=1)\n","\n","print(classification_report(testY.argmax(axis=1), predIdxs,\n","\ttarget_names=lb.classes_))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       covid       0.88      0.41      0.56        17\n","      normal       0.98      1.00      0.99       400\n","\n","    accuracy                           0.97       417\n","   macro avg       0.93      0.70      0.77       417\n","weighted avg       0.97      0.97      0.97       417\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irgvVeEJ5vfs","executionInfo":{"status":"ok","timestamp":1638303748856,"user_tz":300,"elapsed":10777,"user":{"displayName":"Daxin Niu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0tgVh9lroul4cj-lkayJFBCJyxhSKif2sV9PnqA=s64","userId":"17158397741899558228"}},"outputId":"caf2b681-fbbd-40f0-ca9c-62a66510cb37"},"source":["predIdxs = np.argmax(model.predict(test_data, batch_size=BATCH_SIZE), axis=1)\n","\n","print(classification_report(test_labels.argmax(axis=1), predIdxs,\n","\ttarget_names=lb.classes_))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","       covid       0.96      0.81      0.88        84\n","      normal       0.99      1.00      1.00      2000\n","\n","    accuracy                           0.99      2084\n","   macro avg       0.97      0.90      0.94      2084\n","weighted avg       0.99      0.99      0.99      2084\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"JXxRMQ6yfcLU"},"source":["We can also compute the confusion matrix to find the sensitivity and specificity."]}]}